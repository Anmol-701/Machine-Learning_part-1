{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b31067a-0814-4f1a-99b3-b15dbdb70eb0",
   "metadata": {},
   "source": [
    "**Problem 1: \n",
    "Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30551fb5-eba0-4e24-87b1-14fbef4bfa0c",
   "metadata": {},
   "source": [
    "Solution 1: \n",
    "-  Simple linear regression predicts the dependent variable using only one independent variable.It is useful to understand the relationship between a single factor and the outcome.\n",
    "-  Example - predicting house price based on the size of the house(single factor)\n",
    "-  Multiple linear regression predicts the dependent variable based on two or more independent variables. It is useful to predict the outcome when multiple factors are involved.\n",
    "-  Example - predicting house price based on size, number of rooms, other features like balcony,windows,etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b81bc-af97-4e7e-81e5-503b44d60c87",
   "metadata": {},
   "source": [
    "**Problem 2: \n",
    "Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfccea7-e54b-42b0-8e52-04ca37ef7c81",
   "metadata": {},
   "source": [
    "Solution 2:\n",
    "-  assumption 1 - The relationship between the independent and the dependent varible is linear.\n",
    "Scatter plot is helpful to check linear trend.\n",
    "-  assumption 2 - Homoscedasticity means that the spread of the errors should be relatively uniform, regardless of the value of the predictor.\n",
    "To check it plot the residuals against the predicted values. The spread of residuals should remain constant for all levels of the fitted values.\n",
    "-  assumption 3 - Normality - The residuals should follow a normal distribution when considering multiple predictors together.\n",
    "Histogram can be used to check the normality.\n",
    "-  assumption 4 - Independence of error - It means errors associated with one observation should not influence the error of any other observation.\n",
    "Random plot can he be helpful to check the independence of error if there is not clear pattern or correlation over time.\n",
    "-  assumption 5 - No multicollinearity - when two or more independent variable in the model are highly correlated, leading to redundancy in the information they provide. This can affect the errors of coefficients, making it difficult to determine the effect of each independent variable.\n",
    "This can be checked using correlation matrix \n",
    "-  assumption 6 - No significant outliers - There are no extreme outliers that affect the model's performance.\n",
    "Box plot can help to check the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86627cdf-5a87-468a-b5a0-06460d9ab630",
   "metadata": {},
   "source": [
    "**Problem 3:\n",
    "How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88a86c-eb76-4ed1-9ca7-638abfa5b1e3",
   "metadata": {},
   "source": [
    "Solution 3:\n",
    "To predict the sales revenue based on expenditure :\n",
    "Sales Revenue=5000+20×(Advertising Expenditure)\n",
    "-  Intercept suggests that when the company spends 0 rupees on advertising,the predicted sales revenue is 5000 rupees.\n",
    "-  slope suggests that for each extra rupees spent on advertising, the sales revenue is expected to increase by 20 rupees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80020037-b7f2-4b7e-bde7-f7eb9f2684c9",
   "metadata": {},
   "source": [
    "**Problem 4:\n",
    "Explain the concept of gradient descent. How is it used in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835093e-89e8-45e6-b465-e074bf4d4124",
   "metadata": {},
   "source": [
    "Solution 4:\n",
    "-  Gradient Descent is an optimization algorithm that is used to minimie the loss function in machine learning.\n",
    "-  The goal of gradient descent is to find the best paramenters for a model by iteratively adjusting them to reduce the error between the predicted and actual values.\n",
    "-  First parameter are initialized, then gradients are computed and in third step the parameters are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fbc32-5113-4e5e-a38c-03ec3a9bebc7",
   "metadata": {},
   "source": [
    "**Problem 5:\n",
    "Describe the multiple linear regression model. How does it differ from simple linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d53fec-fbe4-4530-bea5-28a62972c9de",
   "metadata": {},
   "source": [
    "Solution 5:\n",
    "-  Multiple linear regression is extension of simple linear regression that models the relationship between the a dependent variable and two or more independent variables.\n",
    "Differences :\n",
    "- simple linear regression has single independent variable while multiple linear regression has multiple independent   variables.\n",
    "- Simple linear regression is easy to interpret while multiple linear regression is little complex because it accounts for the joint effect of multiple independent variables on dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71073e8-c075-4301-a959-b8743a17205a",
   "metadata": {},
   "source": [
    "**Problem 6:\n",
    "Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54500ac8-8299-475f-b88c-90430fafe1a6",
   "metadata": {},
   "source": [
    "Solution 6:\n",
    "- Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. When multicollinearity exists,it can cause issues in estimating the relationship between the dependent and independent variables accurately.\n",
    "- It can be detected using correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17087d2b-e4db-4119-b81c-60c7ab1c4ea2",
   "metadata": {},
   "source": [
    "**Problem 7:\n",
    "Describe the polynomial regression model. How is it different from linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7621e79-074c-411a-b794-16cdb7867bf9",
   "metadata": {},
   "source": [
    "Solution 7:\n",
    "-  Polynomial regression is extension of linear regression model that models the relationship between the independent variables and the dependent variable using polynomial function, rather than a simple straight line.\n",
    "-  Linear regression can only model linear relationships while the polynomial regression models non linear relationships.\n",
    "-  Linear regression is simple, easy to interpret and computationally less demanding. On the other hand polynomial regression is more complex and involves higher-order terms and if the degree is too high it can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40335f1-608b-4d35-b3a0-f72ecd20ffec",
   "metadata": {},
   "source": [
    "**Problem 8:\n",
    "What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e79f0-15a0-45d5-8174-c896bf346605",
   "metadata": {},
   "source": [
    "Solution 8 :\n",
    "Advantages of polynomial regression over linear regression\n",
    "-  Polynomial regression is more flexible and can model non linear relationships whereas linear regression assumes a straight-line relationship.\n",
    "-  Polynomial regression is better fit for curved dataset.\n",
    "-  By adding higher degree polynomial terms, polynomial regression can better adapt to the shape of the data,providing more accurate predictions.\n",
    "\n",
    "Disadvantages:\n",
    "-  Polynomial regression is more prone to overfitting if the degree is too high.\n",
    "-  As polynomial degree increases, the model becomes more complex and harder to interpret.\n",
    "-  If polynomial degree is too high, polynomial regression may generalize poorly to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
