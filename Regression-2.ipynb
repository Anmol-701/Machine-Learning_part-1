{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad08183-f443-464c-9c80-d45e9ca22af7",
   "metadata": {},
   "source": [
    "# Problem 1: Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f5555-24ac-40a8-b944-fbe3ab37f02f",
   "metadata": {},
   "source": [
    "Solution: \n",
    "-  R-squared is a statistical measure that represents the goodness of fit of a regerssion model.The value of R-square lies between 0 and 1. When R-square equals 1 the model perfectly fits the data and there is no difference between the predicted value and the actual value.\n",
    "-  R-squared is calculated by subtracting the (sum of squares of residual/total sum of squares) from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d31788-0fd5-4974-8f93-db030d113953",
   "metadata": {},
   "source": [
    "# Problem 2: Define adjusted R-squared and explain how it differs from the regular R-squared.Â "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e1bad-dcd7-43d9-8335-850c88239764",
   "metadata": {},
   "source": [
    "Solution :\n",
    "-  Adjusted R-squared is a measure similar to R-squared that is used to measure the goodness of fit of regression, it accounts for the number of predictors in the model, providing a more accurate measure of the model's explanatory power when multiple variables are involved.\n",
    "-  R-square measure the proportion of the vairance in the dependent variable that is predictable from the independent variables while Adjusted R-square adjusts the R-square value by penalizing the addition of unnecessary predictors.It accounts for both the goodness of fit and the number of predictors used.\n",
    "-  R-square can only increase or stay same when more predictors are added to the model, even if those predictors do not actually improve the model.This can lead to overfitting.Adjusted R-square decreases if the addition of a new predictor does not improve the model enough to justify the extra complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64173b-51ce-4700-8244-fd9005b9d0e4",
   "metadata": {},
   "source": [
    "# Problem 3: When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5af704-0b8f-4482-9fc7-068c81738764",
   "metadata": {},
   "source": [
    "Solution :\n",
    "- It can be used to compare the models with a different number of independent variables.\n",
    "- In models with many independent variables, especially when the number of predictors approaches the number of observations, there is a high risk of overfitting.\n",
    "- If too many predictors are added to model,it may become complex and overfit the training data, which harms the model's generalizability.\n",
    "- Adjusted R-square is better indicator of a model's predictive accuracy when you're trying to choose the best model for future data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9b5a3-3a3b-40fd-b32b-a6d064fecec6",
   "metadata": {},
   "source": [
    "# Problem 4: What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92d22d-92be-49c7-8790-6fec05b29e94",
   "metadata": {},
   "source": [
    "Solution :\n",
    "- In terms of regression analysis RMSE,MSE and MAE are the metrics used to evaluate the performance of a regression model.\n",
    "- RMSE is the square root of the average of the squared of the differences between the observed and predicted values.It provides a measure of the average magnitude of the error in the model's predictions, with the error expressed in the same units as the dependent variable.\n",
    "- MSE is the average of the squared difference between the observed and predicted values.Like RMSE, it penalizes larger errors more heavily but is expressed in the square of the units of the dependent variable.\n",
    "- MAE is the average of the absolute difference between the observed and predicted values. Unlike, MSE and RMSE,MAE does not penalize large errors more heavily than small ones, making it a simpler, more interpretable metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bca9fe-7573-4e2a-be7b-9ce805d49cea",
   "metadata": {},
   "source": [
    "## Problem 5: Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1e257-9c48-478f-9bb0-18309514132e",
   "metadata": {},
   "source": [
    "Solution :\n",
    "\n",
    "Advantages of RMSE:\n",
    "\n",
    "- RMSE is expressed in the same units as the dependent variable, making it easy to interpret in the context of the problem.\n",
    "- RMSE gives more weight to large errors due to the squaring of residuals.This makes it useful when large error are undesirable.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "- Since RMSE squares the residuals, it is sensitive to outliers.\n",
    "- It's scale is tied to the units of the dependent variable , which can make it difficult to compare accross different datasets or models unless the scales are similar.\n",
    "- Because it gives more weight to large errors, RMSE might exaggerate the importance of small issues, especially when the problem doesn't inherently require such a strong emphasis on large errors.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "- MSE is easy to calculate and derive from basic principles.\n",
    "- Penalizes large errors more than smaller ones due to the squaring of residuals.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "- MSE is expressed in squared units of the dependent variable.\n",
    "- Difficult to compare accross different datasets or problems.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "- Easy to interpret as it is expressed in the same units as dependent varibale.\n",
    "- Roust to outliers, treat all errors equally.\n",
    "- Doesn't emphasize large errors as much as RMSE and MSE.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "- Less sensitive to large errors.\n",
    "- Not commonly used for optimization in certain algorithms.\n",
    "- Non-differentiable at zero,causing optimization challenges for certain models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fb3a2-20c5-4932-ab33-3260e9d36041",
   "metadata": {},
   "source": [
    "## Problem 6: Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ebd72-c496-443c-a30f-d50408d2f644",
   "metadata": {},
   "source": [
    "Solution :\n",
    "- Lasso Regression is a technique used in linear regression models to improve their performance by preventing overfitting. It works by adding a penalty term to the loss function that shrinks the coefficients of the model, effectively \"shrinking\" some coefficients to zero. This means that lasso performs both regularization and feature selection.\n",
    "- Lasso uses the L1 norm as its penalty term. This causes some coefficients to be exactly zero. Ridge uses the L2 norm as its penalty term, which causes the coefficients to shrink towards zero but does not make them exactly zero.\n",
    "- Lasso eliminates some features and perform feature selection.Ridge regression does not eliminate features, but retain them with smaller magnitudes.\n",
    "- Lasso is appropriate to use when irrelevant features need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3597c32-98f2-45a6-916f-3a3e7225b18f",
   "metadata": {},
   "source": [
    "## Problem 7: How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5f54e-ca77-44fa-b06a-24a3bae627b8",
   "metadata": {},
   "source": [
    "Solution:\n",
    "- Regulaized linear models help prevent overfitting in machine learning by adding penalty term to the model's complexity. Overfitting occurs when the model learns not only the underlying patterns in the training data but also the noise or random fluctuations.\n",
    "- Regularization prevents the model from relying too much on any single feature by adding a penalty for large coefficients, thus preventing overfitting.\n",
    "- with regularization, model is encourgaed to use simpler, more general relationships between the features and the target variable.\n",
    "- For example, building a linear regression model to predict the house prices based on various features like square footage, number of bedrooms, and year built.Suppose the training data has some noise, such as unusual outliers which could lead the model to fit those outlier too closely, resulting in overfitting. Here comes regularization into picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0586e-7df1-4669-af65-e4e1d2d67890",
   "metadata": {},
   "source": [
    "# Problem 8: Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe3caa-5c6c-4f36-ace1-23c7bf067ede",
   "metadata": {},
   "source": [
    "Solution:\n",
    "- Regularized linear models assume that relationship between the features and the target variable is linear. So in cases where the true relationship is highly non linear, a regularized linear model may struggle to capture the pattern.\n",
    "- Regularized linear models are sensitive to the scale of features. Without proper scaling, the model might penalize features with smaller scales less than those with larger scales.\n",
    "- In cases where multiple features are highly correlated, lasso tends to select one feature at the expense of the others. This can result in a biased model, where important features are discarded or left out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12e42e-6e54-43e5-a84e-87105f4364b9",
   "metadata": {},
   "source": [
    "# Problem 9: You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c3cd6-852d-4971-8d9d-468ccb63f8ca",
   "metadata": {},
   "source": [
    "Solution:\n",
    "- To compare that it depends on the how one care about large errors and want to penalize them more then model A with RMSE of 10 might be preferable.\n",
    "- Otherwise, if one want a model that gives equal importance to all the errors and is more robust to outiers, model B with the MAE of 8 could be a better choice.\n",
    "  \n",
    "Limitations:\n",
    "- RMSE is sensitive to outliers, it might not be the best choice if dataset contains outliers that don't reflect the typical behaviour of data.\n",
    "- MAE is more robust to outliers, so it might not fully capture the variance or spread of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f6fe1-2bc9-4695-bbf3-24b6a839b739",
   "metadata": {},
   "source": [
    "# Problem 10:You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e66bb-8419-494d-9c94-f84b26a48603",
   "metadata": {},
   "source": [
    "Solution:\n",
    "- If feature selection is important and many features are irrelevant Model B might be a better choice. It has high regularization parameter (0.5), which will lead to stronger regularization and more feature elimination.This might be helpful if you believe many features are not contributing the the prediction,but it could also result in underfitting if set too high.\n",
    "- If most or all features contribute to the prediction and want to preserve as much information as possible, Model A might perform better. Ridge can help regularize the coefficients without eliminating any features,making it suitable for cases where all the features are useful.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "\n",
    "Ridge (pros and cons):\n",
    "- Works well when features are correlated, maintains all the features int he model.\n",
    "- But doesn't perform feature selection.\n",
    "  \n",
    "Lasso (pros and cons):\n",
    "- Perform feature seleciton by setting irrelevant coefficients to zero, making the model simpler and more interpretable.\n",
    "- But can lead to underfitting if the regularization parameter is too large or if too many features are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9881ab-6e9e-4b3b-86e6-92bb39f5e9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
