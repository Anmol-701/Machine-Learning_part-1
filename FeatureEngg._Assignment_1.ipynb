{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a51b41a-ab21-4b46-beb1-4fcc025aebef",
   "metadata": {},
   "source": [
    "### Problem_1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91f16e-f58e-4d6a-bdb5-1b6f5adcadec",
   "metadata": {},
   "source": [
    "Missing values in a dataset represent data points that are absent for specific variables in a row. They can appear as blank cells, null values, or special symbols.\n",
    "\n",
    "Here's why handling missing values is crucial:\n",
    "  - Reduced Sample Size: Missing values can decrease the number of usable data points, impacting the reliability of analysis.\n",
    "  - Biased Results: If missing data is not random, it can skew the results of your analysis and lead to inaccurate conclusions.        \n",
    "  \n",
    "However, some algorithms can handle missing values more gracefully than others:\n",
    "  - Decision Trees: These algorithms can inherently handle missing values by splitting data based on available features. They can simply ignore rows with missing values during splitting.\n",
    "  - K-Nearest Neighbors (KNN): KNN can estimate missing values based on the features of similar data points (nearest neighbors).\n",
    "  - Random Forests: By combining multiple decision trees, random forests can also address missing values through the voting mechanism during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d99bf-3d9e-4f8f-83b6-c7be3e4d8b94",
   "metadata": {},
   "source": [
    "### Problem_2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8946a6f-5cea-4c94-a363-43b0a2ef742a",
   "metadata": {},
   "source": [
    "1. Deletion: This involves removing rows or columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8ba2f5-5afe-4d26-8442-7c77b3667f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C\n",
      "3  4.0  8.0  12.0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8], 'C': [None, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values (axis=0 for rows)\n",
    "df_dropna = df.dropna()\n",
    "\n",
    "# Drop columns with missing values (axis=1 for columns)\n",
    "df_dropna_columns = df.dropna(axis=1)\n",
    "\n",
    "print(df_dropna)\n",
    "print(df_dropna_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa6da6-e06b-46ac-85ed-da984c314c98",
   "metadata": {},
   "source": [
    "2. Imputation: This involves replacing missing values with estimated values.\n",
    "   - Mean/Median Imputation: Replace missing values with the mean (numerical) or median (categorical) of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98647e0a-d500-4442-915b-7bc40ab7ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B          C\n",
      "0  1.000000  5.000000   2.333333\n",
      "1  2.000000  2.333333  10.000000\n",
      "2  2.333333  7.000000  11.000000\n",
      "3  4.000000  8.000000  12.000000\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean (replace with median for categorical data)\n",
    "df_fillna_mean = df.fillna(df['A'].mean())\n",
    "\n",
    "print(df_fillna_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fb043-04b7-4441-893c-635a91044e0f",
   "metadata": {},
   "source": [
    "3. Interpolation: This involves estimating missing values based on surrounding values.\n",
    "   - Linear Interpolation: Estimate missing values by fitting a line between neighboring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175a614d-7e75-4d30-9a17-7fc86077d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C\n",
      "0  1.0  5.0   NaN\n",
      "1  2.0  6.0  10.0\n",
      "2  3.0  7.0  11.0\n",
      "3  4.0  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "# Interpolate missing values linearly (may not be suitable for all data)\n",
    "df_interpolate = df.interpolate('linear')\n",
    "\n",
    "print(df_interpolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e06c7-7b4e-4145-981b-5aa7c23d17ce",
   "metadata": {},
   "source": [
    "### Problem_3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3054cc-573c-480a-a5fe-e86d47635a88",
   "metadata": {},
   "source": [
    "Imbalanced data refers to situations in machine learning, particularly classification tasks, where there's a significant difference in the number of data points belonging to different classes. Imagine a dataset classifying emails as spam or not spam. If 99% of emails are normal and only 1% is spam, that's imbalanced data.\n",
    "\n",
    "Here's what can happen if you don't handle imbalanced data:\n",
    "   - Misleading Accuracy: Standard accuracy metrics can be misleading. A model might just predict the majority class (normal emails) most of the time and achieve high accuracy, but it would completely miss the rare and crucial minority class (spam).\n",
    "   - Poor Performance for Minority Class: The model might not learn the patterns specific to the minority class effectively, leading to poor performance in identifying those crucial cases (failing to detect spam emails)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e7fec-7601-4c7f-8cc6-f4b921309cfb",
   "metadata": {},
   "source": [
    "### Problem_4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fe0a1-93ce-463c-adc3-e09c93e2c30b",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address imbalanced class distributions in machine learning. They work by manipulating the training data to create a more balanced representation of the classes.\n",
    "\n",
    "  - Up-sampling: This increases the number of data points in the minority class.\n",
    "      - Example: Imagine a dataset with 90% cat images and 10% dog images. Up-sampling would involve duplicating data points from the dog class to create a more balanced dataset for training your image classifier.\n",
    "  - Down-sampling: This decreases the number of data points in the majority class.\n",
    "      - Example: Continuing with the cat vs. dog image data, down-sampling would involve randomly removing data points from the cat class to match the number of dog images.         \n",
    "      \n",
    "Here's when you might use each technique:\n",
    "\n",
    "  - Up-sampling is preferred when:\n",
    "     - There's a small amount of data available for the minority class, and acquiring more data is difficult or expensive.\n",
    "     - The minority class is crucial for the model's performance (e.g., detecting rare diseases).\n",
    "  - Down-sampling is preferred when:\n",
    "     - Duplicating data in the minority class might lead to overfitting, as you're essentially creating copies of existing data points.\n",
    "     - The majority class data is large and computational resources for training are limited (down-sampling reduces training time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793620c-2bad-4314-8a71-f516555290ec",
   "metadata": {},
   "source": [
    "### Problem_5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498538b4-b684-4b5c-bbad-2883baa206fe",
   "metadata": {},
   "source": [
    "Data augmentation is a technique specifically used to artificially increase the size and diversity of your training data for machine learning tasks, particularly computer vision. It works by creating new variations of existing data points through various transformations.\n",
    "\n",
    "Here's the idea: Imagine you have a training dataset with images of cats. Data augmentation might involve techniques like:\n",
    "  - Flipping the image horizontally (creating a mirrored image)\n",
    "  - Rotating the image slightly\n",
    "  - Adding small random brightness or contrast changes\n",
    "  - These variations help the model learn the underlying features of cats (eyes, whiskers, etc.)  better and become more robust to slight variations in real-world images.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) is a specific data augmentation technique designed for imbalanced classification problems. Unlike general data augmentation, SMOTE focuses on the minority class. Here's what it does:\n",
    "1. Identify data points from the minority class.\n",
    "2. For each minority class data point, find its nearest neighbors (similar data points).\n",
    "3. Randomly select one of the nearest neighbors.\n",
    "4. Create a new synthetic data point by interpolating (taking a weighted average) between the original data point and the selected neighbor.      \n",
    "\n",
    "This essentially creates new, synthetic minority class data points based on existing ones, helping to balance the class distribution for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecb06c-23be-4542-830a-abb328ed199f",
   "metadata": {},
   "source": [
    "### Problem_6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970e986-f1d0-425a-8b20-4f1920ad1512",
   "metadata": {},
   "source": [
    "In machine learning, outliers are data points that fall significantly outside the typical range of the other data points in a dataset. Imagine a dataset tracking house prices, with most houses priced between dollar200,000 and dollar500,000. A data point showing a house price of $10 million would be considered an outlier.\n",
    "\n",
    "Here's why handling outliers is important:\n",
    "  - Distorted Results: Outliers can significantly skew the results of statistical analysis and machine learning models. For example, if you're trying to predict average house prices, that 10 million dollar outlier would throw off the average and make it an inaccurate representation of the typical house price.\n",
    "  - Misleading Inferences: Outliers can lead to misleading conclusions if not investigated. The 10 million dollar house price might be a data entry error or a luxurious mansion in a specific neighborhood, and simply excluding it without understanding why could lead to a missed insight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9f8b0-1d2c-4822-a6ae-18dfb77bcbf9",
   "metadata": {},
   "source": [
    "### Problem_7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320b913-e4bb-44b9-a190-518c1ec0f6cc",
   "metadata": {},
   "source": [
    "Since you're dealing with missing data in customer analysis, here are some techniques you can consider:\n",
    "\n",
    "1. Understand the Missingness: Analyze why the data is missing. Is it random (e.g., some users skip optional fields) or systematic (e.g., a technical issue causing data loss)? This understanding will guide your approach.\n",
    "\n",
    "2. Deletion (if minimal): If the amount of missing data is minimal (less than 5%) and missingness is random, you might be able to simply remove rows/columns with missing values. However, this reduces data size and may introduce bias.\n",
    "\n",
    "3. Imputation: This involves filling in missing values with estimates. Here are some customer data-specific options:\n",
    "    - Mean/Median: Replace missing values with the average (numerical) or median (categorical) value for that specific customer attribute (e.g., average purchase amount).\n",
    "   - Mode: Replace missing values with the most frequent value for that attribute (e.g., most frequent product category purchased).\n",
    "4. Model-based Techniques: For complex scenarios, consider using machine learning models to predict missing values based on available customer data. This can be more accurate than simple imputation techniques.\n",
    "\n",
    "5. Feature Engineering (if applicable): Create new features based on existing data that might help predict missing values. For example, if income is missing, you could create a feature based on zip code and profession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7306e8b-d60b-4379-a073-4b940be64438",
   "metadata": {},
   "source": [
    "### Problem_8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef2ec6-d300-4542-a2e4-e15d745d239a",
   "metadata": {},
   "source": [
    "To see if missing data (even a small %) is random in your large dataset:\n",
    "1. Visualize Missingness: Are missing values concentrated in specific areas? Random would be more spread out.\n",
    "2. Compare Distributions: Do key statistics (mean/median) differ between complete and missing data points?\n",
    "3. Simple Tests: Run Chi-Square or Logistic Regression to see if missingness is related to other data.\n",
    "4. Domain Knowledge: Consider reasons for missingness based on data collection.           \n",
    "\n",
    "These checks can help you decide if the missing data is random or patterned, influencing how you handle it for better analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff78bc-ce76-41bd-b018-dbd8a64e8cc3",
   "metadata": {},
   "source": [
    "### Problem_9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a3970-90e6-4164-be25-40a8a2fce7a8",
   "metadata": {},
   "source": [
    "Evaluating a Medical Diagnosis Model on Imbalanced Data:\n",
    "- Metrics:\n",
    "  - Use precision, recall, F1-score to assess performance on both majority (healthy) and minority (diseased) classes.\n",
    "  - Consider ROC-AUC to evaluate the model's ability to discriminate between them.\n",
    "- Techniques:\n",
    "  - Employ a confusion matrix to visualize prediction accuracy for each class.\n",
    "  - Utilize cost-sensitive learning or threshold tuning to focus on accurate minority class prediction.\n",
    "- Addressing Imbalance:\n",
    "  - Stratified evaluation ensures test data reflects the real-world class distribution.\n",
    "  - Compare your model to a baseline (e.g., predicting all negative) to assess its effectiveness in identifying the rare condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dbe7e-77dd-4755-85c3-91b040680780",
   "metadata": {},
   "source": [
    "### Problem_10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11cb447-2ae5-4d5e-9936-79abba4ae414",
   "metadata": {},
   "source": [
    "Here are methods to balance your customer satisfaction dataset and down-sample the majority class (satisfied customers):\n",
    "\n",
    "Down-sampling Techniques:\n",
    "  - Random Down-sampling: This randomly removes satisfied customer entries until the number matches (or roughly balances) the number of dissatisfied entries. It's simple but might discard valuable data.\n",
    "  - Stratified Down-sampling: This maintains the proportion of satisfied customers within different subgroups (e.g., product category, demographics). It preserves representativeness but requires knowledge of subgroups.\n",
    "  - Nearest Neighbor Down-sampling: Remove satisfied customer entries that are most similar (closest features) to existing dissatisfied entries. This focuses on diverse satisfied data points for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faee2b8-9b17-4fa3-be8d-d3f80a54601f",
   "metadata": {},
   "source": [
    "### Problem_11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab9ce4-f1bf-49fd-a5d0-26afb650e238",
   "metadata": {},
   "source": [
    "Up-sampling Techniques:\n",
    "  - Random Up-sampling: This simply duplicates existing minority class entries to increase their representation. It's easy but can lead to overfitting as you're copying existing data.\n",
    "  - SMOTE (Synthetic Minority Oversampling Technique): This creates new, synthetic data points for the minority class by interpolating between existing minority class entries and their nearest neighbors. It injects more diverse variations without simply copying data.\n",
    "  - ADASYN (Adaptive Synthetic Minority Oversampling Technique): Similar to SMOTE, but focuses on creating synthetic data points for areas with lower minority class density, addressing potential overfitting issues in certain data distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
