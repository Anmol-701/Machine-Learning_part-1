{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983527c1-636b-41f9-9905-57a16f497fc0",
   "metadata": {},
   "source": [
    "### Problem_1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784ca8c-0d20-4cf8-8047-205d8b04a037",
   "metadata": {},
   "source": [
    "- Min-Max scaling is a technique in data preprocessing that rescales features in your data to a specific range, often between 0 and 1. This ensures all features contribute equally to machine learning algorithms that might be sensitive to the scale of the data.\n",
    "\n",
    "- Imagine you have a dataset with income (in dollars) and age (in years). Income can range from very low to very high, while age has a much tighter range. Min-Max scaling would transform both features to a 0-1 range, making them comparable for the machine learning model.\n",
    "\n",
    "Here's a simplified example:\n",
    "\n",
    "  - Original income: dollar 10,000 - dollar 100,000\n",
    "  - Original age: 20 - 70      \n",
    "  \n",
    "After Min-Max scaling (assuming 0-1 range):\n",
    "  - Scaled income: 0 - 1\n",
    "  - Scaled age: 0 - 1    \n",
    "  \n",
    "This ensures both income and age contribute equally to the model's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f816f74-9e49-4393-8764-0eb5a3c341b3",
   "metadata": {},
   "source": [
    "### Problem_2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645e448-4d16-49d2-be62-2e3057da4474",
   "metadata": {},
   "source": [
    "- The Unit Vector technique in feature scaling transforms each data point into a unit vector, meaning its overall magnitude becomes 1. Unlike Min-Max scaling which focuses on individual feature ranges, Unit Vector considers the entire data point as a single entity in a high-dimensional space.\n",
    "\n",
    "Here's the key difference:\n",
    "  - Min-Max: Scales each feature independently to a specific range (e.g., 0-1).\n",
    "  - Unit Vector: Scales the entire data point (all features combined) to have a magnitude of 1.\n",
    "  \n",
    "Imagine a 2D dataset with points representing locations. Min-Max scaling might stretch or shrink the entire data cloud along the x and y axes independently. Unit Vector, however, would transform each data point into a point on the unit circle, preserving the relative distances between points but not their original locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4448ae-923e-4832-8232-ca495153dd4c",
   "metadata": {},
   "source": [
    "### Problem_3: What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572972a6-6eaa-4883-a2ee-1739d1c5ef28",
   "metadata": {},
   "source": [
    "- Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets by focusing on the most significant variations within the data. It achieves this by creating a new set of features, called principal components (PCs), that capture the most important patterns in the original data. These PCs are essentially new variables formed as linear combinations of the original features.\n",
    "\n",
    "Here's how PCA helps in dimensionality reduction:\n",
    "1. Identifies directions of variance: PCA analyzes the data and identifies directions with the most spread (variance) - these directions hold the most information.\n",
    "2. Creates principal components: It creates new features (PCs) aligned with these directions of variance.\n",
    "3. Reduces dimensionality: By keeping only the first few PCs, which capture the majority of the variance, PCA reduces the overall number of features while retaining the important information.    \n",
    "\n",
    "Example: Imagine a dataset describing customer purchases with features like product category, price, brand, etc. PCA might identify the first PC as capturing the price range preference (expensive vs. budget-friendly) and the second PC capturing brand preference. By keeping these two PCs and discarding less informative ones, you can significantly reduce the number of features for further analysis without losing crucial customer buying behavior patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ca1e5-9313-4e5f-be01-2fc08b10014b",
   "metadata": {},
   "source": [
    "### Problem_4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee187f3-fecb-4e8b-bd7f-f8923dccaade",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is closely tied to feature extraction, and in fact, PCA can be a powerful tool for extracting new, informative features from your data. Here's how it works:\n",
    "\n",
    "- Dimensionality Reduction through Feature Extraction: While PCA's main goal is reducing data complexity, the new features it creates (principal components) often capture the most important underlying variations in the original data. These PCs can be considered extracted features that represent the core information.\n",
    "\n",
    "- Using PCA for Feature Extraction: Here's the process:\n",
    "  1. Apply PCA to your data. It identifies principal components (PCs) as linear combinations of the original features.\n",
    "  2. Analyze the explained variance ratio for each PC. This tells you how much of the total variance each PC captures.\n",
    "  3. Choose a subset of PCs that captures a significant portion of the variance (e.g., 90%). These PCs represent the extracted features.     \n",
    "\n",
    "Example: Imagine a dataset with many features describing images (color, texture, etc.). PCA might extract a smaller set of PCs that capture the most prominent variations in color and texture across the images. These PCs act as new, extracted features that can be used for image classification tasks like identifying objects or scenes, even though they are not the original color or texture values themselves.\n",
    "\n",
    "By using PCA for feature extraction, you end up with a smaller set of informative features that can improve the performance of machine learning models, especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a66ac4-e27e-4a72-90ca-b1c0a1dfe2a1",
   "metadata": {},
   "source": [
    "### Problem_5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e36ac-7872-4e3f-afce-001ed1028c94",
   "metadata": {},
   "source": [
    "Here's how you can use Min-Max scaling to preprocess the data for your food delivery recommendation system:\n",
    "\n",
    "1. Identify Features to Scale: Focus on numerical features that might have significantly different ranges. In this case, price, delivery time (in minutes) are good candidates for Min-Max scaling.\n",
    "2. Ignore Non-Numerical Features:  Leave categorical features like cuisine type or restaurant name unscaled. Min-Max scaling is for numerical values.\n",
    "3. Apply Min-Max Scaling: Use a library like scikit-learn in Python. Here's a basic approach:\n",
    "   - Split your data into training and testing sets.\n",
    "   - Fit the MinMaxScaler on the training data (learn minimum and maximum values).\n",
    "   - Transform both training and testing data using the fitted scaler. This scales prices and delivery times to a common range (e.g., 0-1).       \n",
    "   \n",
    "By Min-Max scaling, you ensure that features like price (which can vary greatly) and delivery time (which might be in minutes) contribute equally to the recommendation system's learning process. This can lead to more balanced recommendations considering both value and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03016c7a-f704-42af-9fca-686ab5f53440",
   "metadata": {},
   "source": [
    "### Problem_6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da20dbd-bd48-485d-beaf-9e69ff1bf814",
   "metadata": {},
   "source": [
    "Here's how you can use PCA to reduce the dimensionality of your stock price prediction dataset:\n",
    "\n",
    "1. Focus on Numerical Features: PCA works best with numerical data. Select relevant numerical features like financial ratios (e.g., P/E ratio, debt-to-equity ratio) and market indicators (e.g., trading volume, volatility).\n",
    "\n",
    "2. Handle Categorical Features: If there are categorical features (e.g., industry sector), consider encoding them numerically (e.g., one-hot encoding) before applying PCA.\n",
    "\n",
    "3. Apply PCA:\n",
    "   - Standardize the data (important for PCA). This ensures all features have similar scales and contribute equally.\n",
    "   - Use a library like scikit-learn to perform PCA. Choose the number of principal components (PCs) to retain based on the explained variance ratio. Aim for a cumulative explained variance ratio that captures a significant portion of the data's variability (e.g., 80-90%).\n",
    "4. Use the PCs for Modeling: Use the extracted principal components (PCs) as new features for your stock price prediction model. These PCs represent the most important underlying factors affecting stock prices based on the original data.\n",
    "\n",
    "By reducing dimensionality with PCA, you can:\n",
    "  - Improve model performance by reducing the risk of overfitting with a large number of features.\n",
    "  - Simplify the model and potentially improve interpretability (depending on how the PCs relate to the original features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d061ba-f4eb-492b-8d78-d6ae03fd0017",
   "metadata": {},
   "source": [
    "### Problem_7: For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e6820-a053-4da5-b37a-e6574ad46f89",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on your data ([1, 5, 10, 15, 20]) to a range of -1 to 1 do these following steps:\n",
    "\n",
    "1. Find the minimum and maximum values in the data:\n",
    "  - Minimum value (min) = 1\n",
    "  - Maximum value (max) = 20        \n",
    "\n",
    "2. Calculate the scaling factor (scale):\n",
    "  - scale = (new_max - new_min) / (old_max - old_min)\n",
    "  - In this case, we want a new range of -1 to 1, so scale = (-1 - 1) / (20 - 1) = -2 / 19        \n",
    "\n",
    "3. Apply the scaling factor to each data point (x):\n",
    "   - Scaled value = (x - min) * scale + new_min For each data point:\n",
    "      - Data point 1: (1 - 1) * (-2/19) + (-1) = -1 (already at the new min)\n",
    "      - Data point 2: (5 - 1) * (-2/19) + (-1) = -0.7368... (rounded to -0.74)\n",
    "      - Data point 3: (10 - 1) * (-2/19) + (-1) = -0.4736... (rounded to -0.47)\n",
    "      - Data point 4: (15 - 1) * (-2/19) + (-1) = -0.2095... (rounded to -0.21)\n",
    "      - Data point 5: (20 - 1) * (-2/19) + (-1) = 0.0526... (rounded to 0.05) \n",
    "      \n",
    "    \n",
    "Therefore, your scaled data becomes: [-1.0, -0.74, -0.47, -0.21, 0.05]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9872ab1-ea41-410b-ac58-803d12cb552a",
   "metadata": {},
   "source": [
    "### Problem_8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3d26f-bc5b-4fb2-866d-3a3ed12342f5",
   "metadata": {},
   "source": [
    "While PCA can be applied to this dataset consisting of height, weight, age, gender (binary), and blood pressure, it's important to note that PCA works best with continuous numerical features. Gender (binary) would need special handling like one-hot encoding before applying PCA.\n",
    "\n",
    "Here's why the number of principal components (PCs) to retain depends on further analysis:\n",
    "\n",
    "1. Interpretability vs. Explained Variance: PCA prioritizes variance.  More PCs capture more variance, but they might become harder to interpret in terms of the original features.\n",
    "\n",
    "2. Data and Domain Knowledge: The number of informative PCs depends on the data itself and your domain knowledge. In this case, with potentially correlated features like height and weight, the first few PCs might capture a significant portion of the variance.\n",
    "\n",
    "3. Elbow Method: A common technique is the elbow method. You plot the explained variance ratio against the number of PCs. The \"elbow\" of the curve often indicates a good stopping point where you capture a high variance ratio without introducing too many PCs.\n",
    "\n",
    "Considering these points, here's a possible approach:\n",
    "1. Preprocess the data (handle gender and potentially standardize numerical features).\n",
    "2. Run PCA and analyze the explained variance ratio for each PC.\n",
    "3. If the first 2-3 PCs capture a high percentage of the variance (e.g., 80% or more), you might choose to retain them for feature extraction, considering the balance between explained variance and interpretability.\n",
    "4. If the elbow method suggests a clear stopping point after a few PCs, that could be your choice.\n",
    "\n",
    "Ultimately, the number of PCs to retain depends on the specific analysis of your data and what best suits your needs for interpretability and capturing the underlying structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
